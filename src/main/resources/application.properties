server.port=8078
spring.application.name=RagApp
spring.servlet.multipart.max-file-size=20MB
spring.servlet.multipart.max-request-size=20MB
spring.threads.virtual.enabled=true
spring.mvc.log-resolved-exception=true
server.error.include-message=always
server.error.include-binding-errors=always
server.error.include-stacktrace=never
server.error.include-exception=false
spring.thymeleaf.cache=false
spring.resources.chain.cache=false
spring.resources.cache.cachecontrol.no-cache=true
spring.jackson.default-property-inclusion=non_null
spring.jackson.mapper.default-view-inclusion=true
management.endpoints.web.exposure.include=*
management.endpoint.shutdown.enabled=true
endpoints.shutdown.enabled=true
spring.jpa.open-in-view=false
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.properties.hibernate.jdbc.batch_size=50
hibernate.jdbc.batch_size=50
#spring.jpa.properties.hibernate.format_sql=true
#https://docs.spring.io/spring-ai/reference/api/embeddings/ollama-embeddings.html
#spring.ai.ollama.base-url=http://mypc2.ai:7869
#spring.ai.ollama.embedding.options.model=KD
#spring.ai.ollama.base-url=http://mypc.ai:7869
spring.ai.ollama.base-url=http://mypc3.ai:7869
spring.ai.ollama.chat.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
#spring.ai.ollama.chat.model=qwen2.5:3b
#spring.ai.ollama.chat.model=deepseek-r1:7b
#spring.ai.ollama.chat.model=qwen2.5:7b
#spring.ai.ollama.chat.model=qwen2.5:14b
spring.ai.ollama.chat.options.temperature=0.5
#spring.ai.ollama.chat.options.topK=40
#spring.ai.ollama.chat.options.mirostatTau=3.0
#spring.ai.ollama.chat.options.stop=nodata_nodata_nodata
spring.ai.ollama.chat.options.numCtx=10240
#spring.ai.ollama.chat.options.numCtx=20480
spring.ai.gemini.chat.options.model=gemini-2.0-flash-exp
spring.ai.gemini.token=000000000000000000000000000000000000000
spring.ai.ollama.transform.options.model=deepseek-r1:32b
#spring.ai.ollama.transform.options.model=qwen2.5:3b
#spring.ai.ollama.transform.options.model=deepseek-r1:7b
spring.ai.ollama.transform.options.temperature=0.3
#spring.ai.ollama.transform.options.topK=30
#spring.ai.ollama.transform.options.mirostatTau=2.0
spring.ai.ollama.transform.options.stop=nodata_nodata_nodata
spring.ai.ollama.transform.options.numCtx=16384
spring.ai.ollama.transform.options.format=json
#spring.ai.ollama.chat.options.temperature=0.6
#spring.ai.ollama.chat.options.topK=10
spring.ai.ollama.embedding.enabled=false
spring.ai.ollama.embedding.model=nomic-embed-text
#spring.ai.ollama.embedding.model=mxbai-embed-large
#spring.ai.ollama.embedding.model=snowflake-arctic-embed
#spring.ai.ollama.embedding.model=rjmalagon/gte-qwen2-1.5b-instruct-embed-f16
#spring.ai.ollama.embedding.temperature=0.6
#spring.ai.ollama.embedding.topK=10
spring.ai.ollama.init.timeout=15m
spring.ai.ollama.init.max-retries=3
spring.ai.retry.max-attempts=3
spring.ai.retry.backoff.multiplier=5
#spring.ai.ollama.embedding.options.model=hf.co/mixedbread-ai/mxbai-embed-large-v1
#spring.ai.ollama.init.pull-model-strategy=always
#spring.ai.ollama.embedding.options.model=llama3.2
#spring.ai.ollama.init.embedding.additional-models[0]=mxbai-embed-large
#spring.ai.ollama.init.embedding.additional-models[1]=nomic-embed-text
#spring.ai.vectorstore.redis.uri=redis://mypc2.ai:6379
#spring.ai.vectorstore.redis.index=kd
#spring.ai.vectorstore.redis.prefix="kd:"
#spring.ai.vectorstore.redis.initialize-schema=true
#spring.redis.database=kd
#spring.data.redis.host=mypc.ai
#spring.data.redis.port=6379
#spring.redis.pool.max-active=8 # Max number of connections that can be allocated by the pool at a given time. Use a negative value for no limit.
#spring.redis.pool.max-idle=8 # Max number of "idle" connections in the pool. Use a negative value to indicate an unlimited number of idle connections.
#spring.redis.pool.max-wait=-1 # Maximum amount of time (in milliseconds) a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely.
#spring.redis.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive.
#spring.redis.timeout=0 # Connection timeout in milliseconds.
spring.datasource.password=pgvector
spring.datasource.username=pgvector
spring.datasource.database-name=pgvector
spring.datasource.url=jdbc:postgresql://mypc3.ai:5432/pgvector?reWriteBatchedInserts=true
#spring.datasource.url=jdbc:postgresql://mypc1.ai:5432/pgvector
#spring.datasource.url=jdbc:postgresql://mypc2.ai:5432/pgvector
spring.flyway.enabled=true
spring.flyway.baseline-on-migrate=true
spring.ai.vectorstore.pgvector.dimensions=384
#spring.ai.vectorstore.pgvector.tableName=vector_store_384
#spring.ai.vectorstore.pgvector.index-type=HNSW
#spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE
#spring.ai.vectorstore.pgvector.dimensions=1536
#spring.ai.vectorstore.pgvector.batching-strategy=TOKEN_COUNT # Optional.Controls how documents are batched for embedding
#spring.ai.vectorstore.pgvector.max-document-batch-size=10000 # Optional.Maximum number of documents per batch
#spring.cache.cache-names=store
#spring.cache.caffeine.spec=maximumSize=500,expireAfterAccess=60m
#logging.level.org.zalando.logbook=TRACE
logging.level.org.springframework.web.filter.CustomRequestLoggingFilter=DEBUG
logging.level.org.springframework.ai.transformer.splitter.TextSplitter=WARN
logging.level.machinum=DEBUG
#logging.level.machinum=INFO
logbook.format.style=curl
logging.file.max-history=1
logging.file.max-size=10MB
logging.file.total-size-cap=10MB
logging.file.name=build/logs/log.txt
app.batch-size=50
app.run-id=${random.uuid}
app.mode=production
app.flow.cooldown=30s
app.http.logs-path=build/http-logs
app.http.logs-enabled=true
#max,min,normal
app.convert-mode=min
# single|springstandard|springmin|lines|whitespaces|balancedlines|balancedsentence|default
app.parallel.enabled=true
app.flow.batch-size=10
#app.cache.type=local
#app.cache.folder=build/cache
app.cache.ttl=14d
assets.cache.folder=build/cache/resources
assets.cache.metadata-file=build/cache/dynamic_cache_metadata.json
app.split.mode=balancedsentence
app.split.overlap=512
app.split.overlap-size=100
# chunks|chunkswithmessage|message|makeuptext|default
#app.history.mode=chunks
app.history.mode=makeupatext
app.compress.mode=simple
app.compress.percentage=50
app.allow-tools=false
app.rewrite.template=Rewrite.ST
app.rewrite.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
#app.rewrite.model=qwen2.5:32b
#app.rewrite.model=gemma2:27b
app.rewrite.temperature=0.8
app.rewrite.numCtx=10240
#app.summary.model=qwen2.5:32b
app.summary.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.summary.temperature=0.3
app.summary.numCtx=10240
app.summary.consolidate.model=qwen2.5:32b
app.summary.consolidate.temperature=0.8
app.summary.consolidate.numCtx=10240
#app.glossary.extract.model=deepseek-r1:32b
app.glossary.extract.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.glossary.extract.temperature=0.3
app.glossary.extract.numCtx=10240
#app.logic-splitter.model=hf.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF:Q5_K_M
app.logic-splitter.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.logic-splitter.temperature=0.8
app.logic-splitter.numCtx=10240
app.logic-splitter.chunkSize=640
app.proofread.en.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
#app.proofread.en.model=gemma2:27b
app.proofread.en.temperature=0.6
app.proofread.en.numCtx=10240
app.proofread.ru.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.proofread.ru.temperature=0.7
app.proofread.ru.numCtx=8192
#app.glossary.translate.model=hf.co/IlyaGusev/saiga_yandexgpt_8b_gguf:BF16
#app.glossary.translate.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.glossary.translate.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.glossary.translate.temperature=0.2
#app.glossary.translate.numCtx=8192
app.glossary.translate.numCtx=10240
app.translate.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.translate.temperature=0.8
app.translate.numCtx=10240
app.translate.provider=ollama
app.translate.fail-on-wrong-chunks=false
#app.translate.title.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.translate.title.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.translate.title.temperature=0.8
app.translate.title.numCtx=4096
#app.translate.scoring.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.translate.scoring.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.translate.scoring.temperature=0.8
app.translate.scoring.numCtx=10240
app.translate.scoring.responseLength=1024
app.translate.score.iterations=2
app.translate.score.quality=9
#app.translate.copy-editing.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.translate.copy-editing.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.translate.copy-editing.temperature=0.8
app.translate.copy-editing.numCtx=10240
#app.translate.copy-editing.numCtx=10240
# ollama | gemini
#app.translate.copy-editing.provider=gemini
app.translate.copy-editing.provider=ollama
# auto | min
app.translate.copy-editing.history.mode=min
#app.translate.copy-editing-scoring.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
app.translate.copy-editing-scoring.model=hf.co/lmstudio-community/gemma-3-27b-it-GGUF:Q6_K
app.translate.copy-editing-scoring.temperature=0.8
app.translate.copy-editing-scoring.numCtx=10240
app.ssml.model=hf.co/t-tech/T-pro-it-1.0-Q6_K-GGUF:latest
#app.ssml.model=hf.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF:q8_0
app.ssml.temperature=0.7
app.ssml.numCtx=8192
app.ssml.chunkSize=1280